{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Word2vec, published by Google in 2013, is a neural network implementation that \n",
    "# learns distributed representations for words. Other deep or recurrent neural\n",
    "# network architectures had been proposed for learning word representations prior to this,\n",
    "# but the major problem with these was the long time required to train the models. \n",
    "# Word2vec learns quickly relative to other models.\n",
    "\n",
    "# Both Google's version and the Python version rely on multi-threading \n",
    "# ln order to train your model in a reasonable amount of time, you will\n",
    "# need to install cython\n",
    "import pandas as pd\n",
    "\n",
    "# Read data from files \n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train Word2Vec it is better not to remove stop words because \n",
    "# the algorithm relies on the broader context of the sentence in \n",
    "# order to produce high-quality word vectors.\n",
    "\n",
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False, remove_nums=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Optionally Remove non-letters\n",
    "    if remove_nums:\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input of Word2Vec is list of lists:\n",
    "# Load the punkt tokenizer\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:336: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:336: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:336: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:336: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:336: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:336: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "    \n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795538"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-03 16:00:41,307 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-03-03 16:00:41,313 : INFO : collecting all words and their counts\n",
      "2019-03-03 16:00:41,314 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-03 16:00:41,370 : INFO : PROGRESS: at sentence #10000, processed 219242 words, keeping 32665 word types\n",
      "2019-03-03 16:00:41,427 : INFO : PROGRESS: at sentence #20000, processed 438623 words, keeping 51663 word types\n",
      "2019-03-03 16:00:41,482 : INFO : PROGRESS: at sentence #30000, processed 651476 words, keeping 66881 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-03 16:00:41,544 : INFO : PROGRESS: at sentence #40000, processed 871114 words, keeping 80990 word types\n",
      "2019-03-03 16:00:41,603 : INFO : PROGRESS: at sentence #50000, processed 1083691 words, keeping 93535 word types\n",
      "2019-03-03 16:00:41,659 : INFO : PROGRESS: at sentence #60000, processed 1298869 words, keeping 104807 word types\n",
      "2019-03-03 16:00:41,717 : INFO : PROGRESS: at sentence #70000, processed 1515513 words, keeping 115640 word types\n",
      "2019-03-03 16:00:41,773 : INFO : PROGRESS: at sentence #80000, processed 1728384 words, keeping 125785 word types\n",
      "2019-03-03 16:00:41,833 : INFO : PROGRESS: at sentence #90000, processed 1945448 words, keeping 136196 word types\n",
      "2019-03-03 16:00:41,893 : INFO : PROGRESS: at sentence #100000, processed 2160633 words, keeping 145760 word types\n",
      "2019-03-03 16:00:41,949 : INFO : PROGRESS: at sentence #110000, processed 2373736 words, keeping 154951 word types\n",
      "2019-03-03 16:00:42,005 : INFO : PROGRESS: at sentence #120000, processed 2589502 words, keeping 164045 word types\n",
      "2019-03-03 16:00:42,066 : INFO : PROGRESS: at sentence #130000, processed 2808001 words, keeping 173092 word types\n",
      "2019-03-03 16:00:42,131 : INFO : PROGRESS: at sentence #140000, processed 3014647 words, keeping 180730 word types\n",
      "2019-03-03 16:00:42,189 : INFO : PROGRESS: at sentence #150000, processed 3233786 words, keeping 189190 word types\n",
      "2019-03-03 16:00:42,246 : INFO : PROGRESS: at sentence #160000, processed 3449518 words, keeping 197400 word types\n",
      "2019-03-03 16:00:42,308 : INFO : PROGRESS: at sentence #170000, processed 3666036 words, keeping 205202 word types\n",
      "2019-03-03 16:00:42,373 : INFO : PROGRESS: at sentence #180000, processed 3880006 words, keeping 212718 word types\n",
      "2019-03-03 16:00:42,437 : INFO : PROGRESS: at sentence #190000, processed 4098458 words, keeping 220316 word types\n",
      "2019-03-03 16:00:42,501 : INFO : PROGRESS: at sentence #200000, processed 4315770 words, keeping 227814 word types\n",
      "2019-03-03 16:00:42,569 : INFO : PROGRESS: at sentence #210000, processed 4530864 words, keeping 235183 word types\n",
      "2019-03-03 16:00:42,638 : INFO : PROGRESS: at sentence #220000, processed 4749178 words, keeping 242719 word types\n",
      "2019-03-03 16:00:42,699 : INFO : PROGRESS: at sentence #230000, processed 4965224 words, keeping 249871 word types\n",
      "2019-03-03 16:00:42,766 : INFO : PROGRESS: at sentence #240000, processed 5186072 words, keeping 256947 word types\n",
      "2019-03-03 16:00:42,825 : INFO : PROGRESS: at sentence #250000, processed 5393721 words, keeping 263710 word types\n",
      "2019-03-03 16:00:42,890 : INFO : PROGRESS: at sentence #260000, processed 5606855 words, keeping 270573 word types\n",
      "2019-03-03 16:00:42,951 : INFO : PROGRESS: at sentence #270000, processed 5821731 words, keeping 277451 word types\n",
      "2019-03-03 16:00:43,018 : INFO : PROGRESS: at sentence #280000, processed 6040767 words, keeping 284594 word types\n",
      "2019-03-03 16:00:43,087 : INFO : PROGRESS: at sentence #290000, processed 6257421 words, keeping 291795 word types\n",
      "2019-03-03 16:00:43,155 : INFO : PROGRESS: at sentence #300000, processed 6475443 words, keeping 298856 word types\n",
      "2019-03-03 16:00:43,223 : INFO : PROGRESS: at sentence #310000, processed 6693957 words, keeping 305881 word types\n",
      "2019-03-03 16:00:43,288 : INFO : PROGRESS: at sentence #320000, processed 6912070 words, keeping 312702 word types\n",
      "2019-03-03 16:00:43,350 : INFO : PROGRESS: at sentence #330000, processed 7127006 words, keeping 319343 word types\n",
      "2019-03-03 16:00:43,412 : INFO : PROGRESS: at sentence #340000, processed 7349526 words, keeping 326367 word types\n",
      "2019-03-03 16:00:43,477 : INFO : PROGRESS: at sentence #350000, processed 7566089 words, keeping 332785 word types\n",
      "2019-03-03 16:00:43,542 : INFO : PROGRESS: at sentence #360000, processed 7780210 words, keeping 339214 word types\n",
      "2019-03-03 16:00:43,608 : INFO : PROGRESS: at sentence #370000, processed 8000734 words, keeping 345893 word types\n",
      "2019-03-03 16:00:43,689 : INFO : PROGRESS: at sentence #380000, processed 8218802 words, keeping 352385 word types\n",
      "2019-03-03 16:00:43,753 : INFO : PROGRESS: at sentence #390000, processed 8441561 words, keeping 358863 word types\n",
      "2019-03-03 16:00:43,812 : INFO : PROGRESS: at sentence #400000, processed 8657763 words, keeping 364962 word types\n",
      "2019-03-03 16:00:43,873 : INFO : PROGRESS: at sentence #410000, processed 8872437 words, keeping 370920 word types\n",
      "2019-03-03 16:00:43,935 : INFO : PROGRESS: at sentence #420000, processed 9086761 words, keeping 377047 word types\n",
      "2019-03-03 16:00:43,994 : INFO : PROGRESS: at sentence #430000, processed 9307298 words, keeping 383392 word types\n",
      "2019-03-03 16:00:44,054 : INFO : PROGRESS: at sentence #440000, processed 9527258 words, keeping 389543 word types\n",
      "2019-03-03 16:00:44,113 : INFO : PROGRESS: at sentence #450000, processed 9744341 words, keeping 395673 word types\n",
      "2019-03-03 16:00:44,176 : INFO : PROGRESS: at sentence #460000, processed 9970201 words, keeping 402122 word types\n",
      "2019-03-03 16:00:44,237 : INFO : PROGRESS: at sentence #470000, processed 10191309 words, keeping 408031 word types\n",
      "2019-03-03 16:00:44,299 : INFO : PROGRESS: at sentence #480000, processed 10405372 words, keeping 413650 word types\n",
      "2019-03-03 16:00:44,368 : INFO : PROGRESS: at sentence #490000, processed 10625269 words, keeping 419851 word types\n",
      "2019-03-03 16:00:44,431 : INFO : PROGRESS: at sentence #500000, processed 10840268 words, keeping 425623 word types\n",
      "2019-03-03 16:00:44,496 : INFO : PROGRESS: at sentence #510000, processed 11058615 words, keeping 431582 word types\n",
      "2019-03-03 16:00:44,562 : INFO : PROGRESS: at sentence #520000, processed 11275224 words, keeping 437401 word types\n",
      "2019-03-03 16:00:44,629 : INFO : PROGRESS: at sentence #530000, processed 11492901 words, keeping 443014 word types\n",
      "2019-03-03 16:00:44,692 : INFO : PROGRESS: at sentence #540000, processed 11710358 words, keeping 449016 word types\n",
      "2019-03-03 16:00:44,761 : INFO : PROGRESS: at sentence #550000, processed 11929146 words, keeping 454633 word types\n",
      "2019-03-03 16:00:44,825 : INFO : PROGRESS: at sentence #560000, processed 12143523 words, keeping 460126 word types\n",
      "2019-03-03 16:00:44,894 : INFO : PROGRESS: at sentence #570000, processed 12365281 words, keeping 465767 word types\n",
      "2019-03-03 16:00:44,959 : INFO : PROGRESS: at sentence #580000, processed 12580155 words, keeping 471302 word types\n",
      "2019-03-03 16:00:45,023 : INFO : PROGRESS: at sentence #590000, processed 12799201 words, keeping 476784 word types\n",
      "2019-03-03 16:00:45,086 : INFO : PROGRESS: at sentence #600000, processed 13014825 words, keeping 481902 word types\n",
      "2019-03-03 16:00:45,145 : INFO : PROGRESS: at sentence #610000, processed 13229257 words, keeping 487440 word types\n",
      "2019-03-03 16:00:45,206 : INFO : PROGRESS: at sentence #620000, processed 13448553 words, keeping 492684 word types\n",
      "2019-03-03 16:00:45,269 : INFO : PROGRESS: at sentence #630000, processed 13666159 words, keeping 497936 word types\n",
      "2019-03-03 16:00:45,332 : INFO : PROGRESS: at sentence #640000, processed 13880391 words, keeping 503161 word types\n",
      "2019-03-03 16:00:45,394 : INFO : PROGRESS: at sentence #650000, processed 14099443 words, keeping 508415 word types\n",
      "2019-03-03 16:00:45,453 : INFO : PROGRESS: at sentence #660000, processed 14315314 words, keeping 513726 word types\n",
      "2019-03-03 16:00:45,520 : INFO : PROGRESS: at sentence #670000, processed 14531988 words, keeping 518638 word types\n",
      "2019-03-03 16:00:45,586 : INFO : PROGRESS: at sentence #680000, processed 14750043 words, keeping 523938 word types\n",
      "2019-03-03 16:00:45,653 : INFO : PROGRESS: at sentence #690000, processed 14965705 words, keeping 528846 word types\n",
      "2019-03-03 16:00:45,722 : INFO : PROGRESS: at sentence #700000, processed 15187631 words, keeping 534264 word types\n",
      "2019-03-03 16:00:45,791 : INFO : PROGRESS: at sentence #710000, processed 15403936 words, keeping 539204 word types\n",
      "2019-03-03 16:00:45,860 : INFO : PROGRESS: at sentence #720000, processed 15622458 words, keeping 544103 word types\n",
      "2019-03-03 16:00:45,926 : INFO : PROGRESS: at sentence #730000, processed 15842196 words, keeping 549093 word types\n",
      "2019-03-03 16:00:45,990 : INFO : PROGRESS: at sentence #740000, processed 16056639 words, keeping 553964 word types\n",
      "2019-03-03 16:00:46,053 : INFO : PROGRESS: at sentence #750000, processed 16268597 words, keeping 558719 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-03 16:00:46,114 : INFO : PROGRESS: at sentence #760000, processed 16481533 words, keeping 563563 word types\n",
      "2019-03-03 16:00:46,176 : INFO : PROGRESS: at sentence #770000, processed 16701810 words, keeping 568626 word types\n",
      "2019-03-03 16:00:46,239 : INFO : PROGRESS: at sentence #780000, processed 16924889 words, keeping 573568 word types\n",
      "2019-03-03 16:00:46,299 : INFO : PROGRESS: at sentence #790000, processed 17144985 words, keeping 578535 word types\n",
      "2019-03-03 16:00:46,335 : INFO : collected 581308 word types from a corpus of 17264346 raw words and 795538 sentences\n",
      "2019-03-03 16:00:46,336 : INFO : Loading a fresh vocabulary\n",
      "2019-03-03 16:00:46,582 : INFO : min_count=40 retains 20587 unique words (3% of original 581308, drops 560721)\n",
      "2019-03-03 16:00:46,582 : INFO : min_count=40 leaves 15706923 word corpus (90% of original 17264346, drops 1557423)\n",
      "2019-03-03 16:00:46,654 : INFO : deleting the raw counts dictionary of 581308 items\n",
      "2019-03-03 16:00:46,668 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2019-03-03 16:00:46,669 : INFO : downsampling leaves estimated 11694391 word corpus (74.5% of prior 15706923)\n",
      "2019-03-03 16:00:46,735 : INFO : estimated required memory for 20587 words and 300 dimensions: 59702300 bytes\n",
      "2019-03-03 16:00:46,736 : INFO : resetting layer weights\n",
      "2019-03-03 16:00:47,054 : INFO : training model with 4 workers on 20587 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-03 16:00:48,066 : INFO : EPOCH 1 - PROGRESS: at 8.93% examples, 1036009 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:00:49,072 : INFO : EPOCH 1 - PROGRESS: at 18.56% examples, 1072655 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:00:50,073 : INFO : EPOCH 1 - PROGRESS: at 28.07% examples, 1084308 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:00:51,074 : INFO : EPOCH 1 - PROGRESS: at 37.61% examples, 1090125 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:00:52,077 : INFO : EPOCH 1 - PROGRESS: at 46.97% examples, 1091951 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:00:53,079 : INFO : EPOCH 1 - PROGRESS: at 56.15% examples, 1088988 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:00:54,092 : INFO : EPOCH 1 - PROGRESS: at 65.44% examples, 1087636 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:00:55,097 : INFO : EPOCH 1 - PROGRESS: at 74.55% examples, 1084450 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:00:56,097 : INFO : EPOCH 1 - PROGRESS: at 83.93% examples, 1085813 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:00:57,099 : INFO : EPOCH 1 - PROGRESS: at 93.50% examples, 1089239 words/s, in_qsize 8, out_qsize 0\n",
      "2019-03-03 16:00:57,739 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-03 16:00:57,744 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-03 16:00:57,746 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-03 16:00:57,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-03 16:00:57,751 : INFO : EPOCH - 1 : training on 17264346 raw words (11694894 effective words) took 10.7s, 1094088 effective words/s\n",
      "2019-03-03 16:00:58,760 : INFO : EPOCH 2 - PROGRESS: at 9.57% examples, 1112859 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:00:59,764 : INFO : EPOCH 2 - PROGRESS: at 18.33% examples, 1061715 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:00,768 : INFO : EPOCH 2 - PROGRESS: at 28.13% examples, 1087098 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:01,769 : INFO : EPOCH 2 - PROGRESS: at 36.29% examples, 1051596 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:02,771 : INFO : EPOCH 2 - PROGRESS: at 45.28% examples, 1051998 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:03,778 : INFO : EPOCH 2 - PROGRESS: at 54.25% examples, 1051235 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:04,780 : INFO : EPOCH 2 - PROGRESS: at 63.89% examples, 1062798 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:05,784 : INFO : EPOCH 2 - PROGRESS: at 71.60% examples, 1042700 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:06,787 : INFO : EPOCH 2 - PROGRESS: at 80.87% examples, 1046619 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:07,787 : INFO : EPOCH 2 - PROGRESS: at 90.02% examples, 1049444 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:08,792 : INFO : EPOCH 2 - PROGRESS: at 98.93% examples, 1048326 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:08,898 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-03 16:01:08,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-03 16:01:08,911 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-03 16:01:08,913 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-03 16:01:08,914 : INFO : EPOCH - 2 : training on 17264346 raw words (11693500 effective words) took 11.2s, 1048090 effective words/s\n",
      "2019-03-03 16:01:09,928 : INFO : EPOCH 3 - PROGRESS: at 8.81% examples, 1022469 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:10,929 : INFO : EPOCH 3 - PROGRESS: at 17.87% examples, 1034571 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:11,932 : INFO : EPOCH 3 - PROGRESS: at 26.98% examples, 1042622 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:12,935 : INFO : EPOCH 3 - PROGRESS: at 36.05% examples, 1044696 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:13,935 : INFO : EPOCH 3 - PROGRESS: at 44.46% examples, 1033405 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:14,938 : INFO : EPOCH 3 - PROGRESS: at 53.52% examples, 1037599 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:15,940 : INFO : EPOCH 3 - PROGRESS: at 62.49% examples, 1040517 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:16,941 : INFO : EPOCH 3 - PROGRESS: at 70.86% examples, 1032730 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:17,945 : INFO : EPOCH 3 - PROGRESS: at 79.83% examples, 1033995 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:18,949 : INFO : EPOCH 3 - PROGRESS: at 88.82% examples, 1035640 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:19,953 : INFO : EPOCH 3 - PROGRESS: at 97.40% examples, 1032132 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:20,242 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-03 16:01:20,247 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-03 16:01:20,255 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-03 16:01:20,258 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-03 16:01:20,258 : INFO : EPOCH - 3 : training on 17264346 raw words (11693550 effective words) took 11.3s, 1031529 effective words/s\n",
      "2019-03-03 16:01:21,278 : INFO : EPOCH 4 - PROGRESS: at 8.86% examples, 1024970 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:22,283 : INFO : EPOCH 4 - PROGRESS: at 17.76% examples, 1023934 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:23,287 : INFO : EPOCH 4 - PROGRESS: at 26.50% examples, 1021564 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:24,288 : INFO : EPOCH 4 - PROGRESS: at 35.36% examples, 1022794 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:25,289 : INFO : EPOCH 4 - PROGRESS: at 43.93% examples, 1019696 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:26,299 : INFO : EPOCH 4 - PROGRESS: at 52.72% examples, 1019343 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:27,304 : INFO : EPOCH 4 - PROGRESS: at 61.39% examples, 1019717 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:28,312 : INFO : EPOCH 4 - PROGRESS: at 70.18% examples, 1019554 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:29,318 : INFO : EPOCH 4 - PROGRESS: at 78.95% examples, 1019771 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:30,324 : INFO : EPOCH 4 - PROGRESS: at 87.45% examples, 1016636 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:31,326 : INFO : EPOCH 4 - PROGRESS: at 96.15% examples, 1016317 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:31,759 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-03 16:01:31,765 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-03 16:01:31,773 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-03 16:01:31,777 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-03 16:01:31,778 : INFO : EPOCH - 4 : training on 17264346 raw words (11695116 effective words) took 11.5s, 1016121 effective words/s\n",
      "2019-03-03 16:01:32,790 : INFO : EPOCH 5 - PROGRESS: at 8.01% examples, 929917 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:33,794 : INFO : EPOCH 5 - PROGRESS: at 16.76% examples, 970145 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:34,799 : INFO : EPOCH 5 - PROGRESS: at 25.41% examples, 980939 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:35,805 : INFO : EPOCH 5 - PROGRESS: at 33.98% examples, 982345 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:36,806 : INFO : EPOCH 5 - PROGRESS: at 42.37% examples, 983214 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:37,818 : INFO : EPOCH 5 - PROGRESS: at 50.73% examples, 980856 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:38,818 : INFO : EPOCH 5 - PROGRESS: at 59.20% examples, 983614 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:39,820 : INFO : EPOCH 5 - PROGRESS: at 67.75% examples, 985462 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:40,825 : INFO : EPOCH 5 - PROGRESS: at 76.19% examples, 985097 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:41,827 : INFO : EPOCH 5 - PROGRESS: at 84.68% examples, 985818 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:42,829 : INFO : EPOCH 5 - PROGRESS: at 93.27% examples, 987651 words/s, in_qsize 7, out_qsize 0\n",
      "2019-03-03 16:01:43,586 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-03 16:01:43,593 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-03 16:01:43,597 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-03 16:01:43,600 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-03 16:01:43,601 : INFO : EPOCH - 5 : training on 17264346 raw words (11695433 effective words) took 11.8s, 989901 effective words/s\n",
      "2019-03-03 16:01:43,602 : INFO : training on a 86321730 raw words (58472493 effective words) took 56.5s, 1034003 effective words/s\n",
      "2019-03-03 16:01:43,603 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-03-03 16:01:43,861 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-03-03 16:01:43,863 : INFO : not storing attribute vectors_norm\n",
      "2019-03-03 16:01:43,864 : INFO : not storing attribute cum_table\n",
      "2019-03-03 16:01:44,460 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('man,', 0.800133228302002),\n",
       " ('man.', 0.7126742601394653),\n",
       " ('woman', 0.6832668781280518),\n",
       " ('soldier', 0.6417524814605713),\n",
       " ('lady', 0.6401040554046631),\n",
       " ('lad', 0.6261498332023621),\n",
       " ('monk', 0.6240255236625671),\n",
       " ('doctor', 0.6199002265930176),\n",
       " ('boy', 0.6128771305084229),\n",
       " (\"man's\", 0.6039730906486511)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('princess', 0.7634856700897217),\n",
       " ('sylvia', 0.7149850130081177),\n",
       " ('queen,', 0.7045449614524841),\n",
       " ('maid', 0.6910901069641113),\n",
       " ('countess', 0.6799722909927368),\n",
       " ('prince', 0.6751097440719604),\n",
       " ('mrs.', 0.6603979468345642),\n",
       " ('bride', 0.6592644453048706),\n",
       " ('belle', 0.6546525955200195),\n",
       " ('maria', 0.6522542238235474)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('atrocious', 0.744181752204895),\n",
       " ('terrible', 0.7220227718353271),\n",
       " ('horrible', 0.7034926414489746),\n",
       " ('awful,', 0.6793063282966614),\n",
       " ('dreadful', 0.6791330575942993),\n",
       " ('abysmal', 0.6705625057220459),\n",
       " ('amateurish', 0.6406466364860535),\n",
       " ('horrid', 0.6367478370666504),\n",
       " ('appalling', 0.635163426399231),\n",
       " ('horrendous', 0.6152829527854919)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-03 16:09:30,218 : INFO : loading Word2Vec object from 300features_40minwords_10context\n",
      "2019-03-03 16:09:30,576 : INFO : loading wv recursively from 300features_40minwords_10context.wv.* with mmap=None\n",
      "2019-03-03 16:09:30,576 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-03-03 16:09:30,577 : INFO : loading vocabulary recursively from 300features_40minwords_10context.vocabulary.* with mmap=None\n",
      "2019-03-03 16:09:30,578 : INFO : loading trainables recursively from 300features_40minwords_10context.trainables.* with mmap=None\n",
      "2019-03-03 16:09:30,578 : INFO : setting ignored attribute cum_table to None\n",
      "2019-03-03 16:09:30,579 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"300features_40minwords_10context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20587, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
